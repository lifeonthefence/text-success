{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6332f439",
   "metadata": {},
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afef1b3d",
   "metadata": {},
   "source": [
    "The first section below uses snscrape to get a list of tweets\n",
    "- I was aiming to try to use this to get tweets to practise preprocessing on but since I'm not sure out dataset will be laid out in the same way (same pandas dataframe) I will jsut try with a dataset of 20,000 random tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f5bdd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (0.5.0.20230113)\n",
      "Requirement already satisfied: filelock in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from snscrape) (3.6.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from snscrape) (4.9.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from snscrape) (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from snscrape) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from beautifulsoup4->snscrape) (2.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (3.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d66cc65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "     -------------------------------------- 636.8/636.8 kB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.5)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42728899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from textblob import TextBlob \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "\n",
    "from argparse import Namespace\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ef1eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7490600",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:elonmusk').get_items()):\n",
    "    if i>=100:\n",
    "        break\n",
    "    tweets_list.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a0c2800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f780888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_1 = df.iloc[0,:] # Selecting the first row of the dataset to analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f2380",
   "metadata": {},
   "source": [
    "# Section 2 - Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337cc56b",
   "metadata": {},
   "source": [
    "Here the NLTK library is used to do data prepocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfb855f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ekmho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "english_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c3b9631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\ekmho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0368c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = twitter_samples.strings(\"tweets.20150430-223406.json\")\n",
    "all_tweets = pd.Series(all_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1131f43d",
   "metadata": {},
   "source": [
    "<b>Finding retweets<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e95916f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tokens = all_tweets.str.split(expand=True)[0]\n",
    "retweeted = all_tweets[all_tweets.str.startswith(\"RT\")].value_counts().iloc[:10] # These are all the retweeted tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab0f64",
   "metadata": {},
   "source": [
    "<b>Filter English tweets</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "textBlob('your tweet').detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b3988",
   "metadata": {},
   "source": [
    "<b> Remove Capitalisation (can leave a marker)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "marker = false\n",
    "for tweet in tweets: \n",
    "    tweet = tweet.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e69504",
   "metadata": {},
   "source": [
    "<b>Clean whitespace, URLS, punctuation, hashtags, emojis</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf3ede",
   "metadata": {},
   "source": [
    "Below it is shown how to remove unwanted phrases or tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6747f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_OF_LINE = r\"^\"\n",
    "OPTIONAL = \"?\"\n",
    "ANYTHING = \".\"\n",
    "ZERO_OR_MORE = \"*\"\n",
    "ONE_OR_MORE = \"+\"\n",
    "\n",
    "SPACE = \"\\s\"\n",
    "SPACES = SPACE + ONE_OR_MORE\n",
    "NOT_SPACE = \"[^\\s]\" + ONE_OR_MORE\n",
    "EVERYTHING_OR_NOTHING = ANYTHING + ZERO_OR_MORE\n",
    "\n",
    "ERASE = \"\"\n",
    "FORWARD_SLASH = \"\\/\"\n",
    "NEWLINES = r\"[\\r\\n]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_TWEET = START_OF_LINE + \"RT\" + SPACES\n",
    "HYPERLINKS = (\"http\" + \"s\" + OPTIONAL + \":\" + FORWARD_SLASH + FORWARD_SLASH\n",
    "              + NOT_SPACE + NEWLINES + ZERO_OR_MORE)\n",
    "HASH = \"#\"\n",
    "tweet = re.sub(RE_TWEET, ERASE, tweet) # tweet is the individual tweet - this can also be done through pandas\n",
    "#Can also sub in any values you want to remove in the above line of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35526554",
   "metadata": {},
   "source": [
    "Here is another way to remove unnecessary words if the data comes as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb1c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the re library\n",
    "token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df['text'].str.lower()\\\n",
    "          .str.replace('(@[a-z0-9]+)\\w+',' ')\\\n",
    "          .str.replace('(http\\S+)', ' ')\\\n",
    "          .str.replace('([^0-9a-z \\t])',' ')\\\n",
    "          .str.replace(' +',' ')\\\n",
    "          .apply(lambda x: [i for i in x.split() if not i in swords])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f096bc57",
   "metadata": {},
   "source": [
    "<b>Stopwords</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a3cd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33760dcb",
   "metadata": {},
   "source": [
    "<b>Tokenisation<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f237641",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(#enter list of tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dd8676",
   "metadata": {},
   "source": [
    "<b>Named Entity Recognition</b>\n",
    "- Same step as removal of unwanted phrases - named entities start with an @"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925452f2",
   "metadata": {},
   "source": [
    "<b>Stemming</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa1347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in #list of clean tweets]\n",
    "#Or with a dataframe it would look like\n",
    "df['stemmed'] = df['processed_text'].apply(lambda x: [ps.stem(i) for i in x if i != ''])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23b956",
   "metadata": {},
   "source": [
    "<b>Remove stopwords and punctuation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4032ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = [word for word in #list of tweets if (word not in english_stopwords and\n",
    "                                       word not in string.punctuation)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ec3ee",
   "metadata": {},
   "source": [
    "<b>Tagging</b> - This labels each word in the tokenized text with word type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "tagged_words = pos_tag(#variable for tokenized words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29435f7c",
   "metadata": {},
   "source": [
    "<b>Lemmatization</b> - This is an alternative way to stemming to normalise words (stemming gets the root of the word, lemmatization use context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f588488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077260c8",
   "metadata": {},
   "source": [
    "# Section 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74939cbd",
   "metadata": {},
   "source": [
    "- Hashtags, URLs and mentions can be extracted during the data cleaning process above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a780bc2",
   "metadata": {},
   "source": [
    "<b>Emojis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1098864",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_count = 0\n",
    "\n",
    "for each character in text: \n",
    "    unicode_num = ord(character)\n",
    "    if (unicode num >= 8986) and if (unicode)num <= 129510): \n",
    "        emoji_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bbe4e0",
   "metadata": {},
   "source": [
    "<b>Sentiment analysis</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab2fa20",
   "metadata": {},
   "source": [
    "<b>Time of day</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa973b4",
   "metadata": {},
   "source": [
    "<b>Speed of tweet after event</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75504d2",
   "metadata": {},
   "source": [
    "<b>Length of text</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aeffcb",
   "metadata": {},
   "source": [
    "<b>TF-IDF: term frequency - inverse document frequency</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e834e",
   "metadata": {},
   "source": [
    "<b>Identifying named people</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d857ecd",
   "metadata": {},
   "source": [
    "One way to do this would be to have a list of key people that we want to check for and then comparing the text tokens to these names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec5228e",
   "metadata": {},
   "source": [
    "<b>User level features</b>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3408453",
   "metadata": {},
   "source": [
    "# Section 3 - Topic modelling trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a22f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages to store and manipulate data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# plotting packages\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# model building package\n",
    "import sklearn\n",
    "\n",
    "# package to clean text\n",
    "import re\n",
    "\n",
    "df = pd.read_csv('climate_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "524dca41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "773"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# make a new column to highlight retweets\n",
    "df['is_retweet'] = df['tweet'].apply(lambda x: x[:2]=='RT')\n",
    "df['is_retweet'].sum()  # number of retweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9c8dfbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9fec3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_links(tweet):\n",
    "    '''Takes a string and removes web links from it'''\n",
    "    tweet = re.sub(r'http\\S+', '', tweet) # remove http links\n",
    "    tweet = re.sub(r'bit.ly/\\S+', '', tweet) # rempve bitly links\n",
    "    tweet = tweet.strip('[link]') # remove [links]\n",
    "    return tweet\n",
    "\n",
    "def remove_users(tweet):\n",
    "    '''Takes a string and removes retweet and @user information'''\n",
    "    tweet = re.sub('(RT\\s@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove retweet\n",
    "    tweet = re.sub('(@[A-Za-z]+[A-Za-z0-9-_]+)', '', tweet) # remove tweeted at\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83627ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_stopwords = nltk.corpus.stopwords.words('english')\n",
    "word_rooter = nltk.stem.snowball.PorterStemmer(ignore_stopwords=False).stem\n",
    "my_punctuation = '!\"$%&\\'()*+,-./:;<=>?[\\\\]^_`{|}~•@'\n",
    "\n",
    "# cleaning master function\n",
    "def clean_tweet(tweet, bigrams=False):\n",
    "    tweet = remove_users(tweet)\n",
    "    tweet = remove_links(tweet)\n",
    "    tweet = tweet.lower() # lower case\n",
    "    tweet = re.sub('['+my_punctuation + ']+', ' ', tweet) # strip punctuation\n",
    "    tweet = re.sub('\\s+', ' ', tweet) #remove double spacing\n",
    "    tweet = re.sub('([0-9]+)', '', tweet) # remove numbers\n",
    "    tweet_token_list = [word for word in tweet.split(' ')\n",
    "                            if word not in my_stopwords] # remove stopwords\n",
    "\n",
    "    tweet_token_list = [word_rooter(word) if '#' not in word else word\n",
    "                        for word in tweet_token_list] # apply word rooter\n",
    "    if bigrams:\n",
    "        tweet_token_list = tweet_token_list+[tweet_token_list[i]+'_'+tweet_token_list[i+1]\n",
    "                                            for i in range(len(tweet_token_list)-1)]\n",
    "    tweet = ' '.join(tweet_token_list)\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4fc176b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['clean_tweet'] = df.tweet.apply(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fab0ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekmho\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# the vectorizer object will be used to transform text to vector form\n",
    "vectorizer = CountVectorizer(max_df=0.9, min_df=25, token_pattern='\\w+|\\$[\\d\\.]+|\\S+')\n",
    "\n",
    "# apply transformation\n",
    "tf = vectorizer.fit_transform(df['clean_tweet']).toarray()\n",
    "\n",
    "# tf_feature_names tells us what word each column in the matric represents\n",
    "tf_feature_names = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "84e854e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "number_of_topics = 10\n",
    "\n",
    "model = LatentDirichletAllocation(n_components=number_of_topics, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e75ae84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LatentDirichletAllocation(random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LatentDirichletAllocation</label><div class=\"sk-toggleable__content\"><pre>LatentDirichletAllocation(random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LatentDirichletAllocation(random_state=0)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "59ad04e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_topics(model, feature_names, no_top_words):\n",
    "    topic_dict = {}\n",
    "    for topic_idx, topic in enumerate(model.components_):\n",
    "        topic_dict[\"Topic %d words\" % (topic_idx)]= ['{}'.format(feature_names[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "        topic_dict[\"Topic %d weights\" % (topic_idx)]= ['{:.1f}'.format(topic[i])\n",
    "                        for i in topic.argsort()[:-no_top_words - 1:-1]]\n",
    "    return pd.DataFrame(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fb8534da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0 words</th>\n",
       "      <th>Topic 0 weights</th>\n",
       "      <th>Topic 1 words</th>\n",
       "      <th>Topic 1 weights</th>\n",
       "      <th>Topic 2 words</th>\n",
       "      <th>Topic 2 weights</th>\n",
       "      <th>Topic 3 words</th>\n",
       "      <th>Topic 3 weights</th>\n",
       "      <th>Topic 4 words</th>\n",
       "      <th>Topic 4 weights</th>\n",
       "      <th>Topic 5 words</th>\n",
       "      <th>Topic 5 weights</th>\n",
       "      <th>Topic 6 words</th>\n",
       "      <th>Topic 6 weights</th>\n",
       "      <th>Topic 7 words</th>\n",
       "      <th>Topic 7 weights</th>\n",
       "      <th>Topic 8 words</th>\n",
       "      <th>Topic 8 weights</th>\n",
       "      <th>Topic 9 words</th>\n",
       "      <th>Topic 9 weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climat</td>\n",
       "      <td>1220.2</td>\n",
       "      <td>global</td>\n",
       "      <td>666.5</td>\n",
       "      <td>global</td>\n",
       "      <td>1147.2</td>\n",
       "      <td>global</td>\n",
       "      <td>473.1</td>\n",
       "      <td>climat</td>\n",
       "      <td>422.0</td>\n",
       "      <td>global</td>\n",
       "      <td>783.0</td>\n",
       "      <td>chang</td>\n",
       "      <td>666.1</td>\n",
       "      <td>warm</td>\n",
       "      <td>167.9</td>\n",
       "      <td>climat</td>\n",
       "      <td>568.3</td>\n",
       "      <td>climat</td>\n",
       "      <td>529.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chang</td>\n",
       "      <td>1184.5</td>\n",
       "      <td>warm</td>\n",
       "      <td>658.1</td>\n",
       "      <td>warm</td>\n",
       "      <td>1102.1</td>\n",
       "      <td>warm</td>\n",
       "      <td>450.7</td>\n",
       "      <td>chang</td>\n",
       "      <td>401.8</td>\n",
       "      <td>warm</td>\n",
       "      <td>764.7</td>\n",
       "      <td>climat</td>\n",
       "      <td>661.6</td>\n",
       "      <td>#climate</td>\n",
       "      <td>139.2</td>\n",
       "      <td>chang</td>\n",
       "      <td>550.5</td>\n",
       "      <td>chang</td>\n",
       "      <td>520.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>via</td>\n",
       "      <td>257.9</td>\n",
       "      <td>snow</td>\n",
       "      <td>160.5</td>\n",
       "      <td>scientist</td>\n",
       "      <td>150.2</td>\n",
       "      <td>believ</td>\n",
       "      <td>101.3</td>\n",
       "      <td>legisl</td>\n",
       "      <td>123.2</td>\n",
       "      <td>gore</td>\n",
       "      <td>137.1</td>\n",
       "      <td>energi</td>\n",
       "      <td>178.8</td>\n",
       "      <td>volcano</td>\n",
       "      <td>128.9</td>\n",
       "      <td>new</td>\n",
       "      <td>321.1</td>\n",
       "      <td>peopl</td>\n",
       "      <td>153.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scienc</td>\n",
       "      <td>112.9</td>\n",
       "      <td>#tcot</td>\n",
       "      <td>121.6</td>\n",
       "      <td>say</td>\n",
       "      <td>87.1</td>\n",
       "      <td>california</td>\n",
       "      <td>87.1</td>\n",
       "      <td>us</td>\n",
       "      <td>105.1</td>\n",
       "      <td>snow</td>\n",
       "      <td>123.7</td>\n",
       "      <td>u</td>\n",
       "      <td>109.8</td>\n",
       "      <td>global</td>\n",
       "      <td>128.6</td>\n",
       "      <td>bill</td>\n",
       "      <td>116.9</td>\n",
       "      <td>world</td>\n",
       "      <td>151.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>news</td>\n",
       "      <td>79.5</td>\n",
       "      <td>like</td>\n",
       "      <td>99.0</td>\n",
       "      <td>scienc</td>\n",
       "      <td>71.7</td>\n",
       "      <td>blame</td>\n",
       "      <td>82.1</td>\n",
       "      <td>via</td>\n",
       "      <td>60.5</td>\n",
       "      <td>al</td>\n",
       "      <td>122.1</td>\n",
       "      <td>via</td>\n",
       "      <td>96.5</td>\n",
       "      <td>chang</td>\n",
       "      <td>122.0</td>\n",
       "      <td>senat</td>\n",
       "      <td>106.1</td>\n",
       "      <td>confer</td>\n",
       "      <td>110.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>day</td>\n",
       "      <td>77.8</td>\n",
       "      <td>blizzard</td>\n",
       "      <td>90.9</td>\n",
       "      <td>debat</td>\n",
       "      <td>66.6</td>\n",
       "      <td>law</td>\n",
       "      <td>78.6</td>\n",
       "      <td>say</td>\n",
       "      <td>55.9</td>\n",
       "      <td>great</td>\n",
       "      <td>94.1</td>\n",
       "      <td>nation</td>\n",
       "      <td>89.0</td>\n",
       "      <td>help</td>\n",
       "      <td>114.1</td>\n",
       "      <td>agenc</td>\n",
       "      <td>95.1</td>\n",
       "      <td>bill</td>\n",
       "      <td>106.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>earth</td>\n",
       "      <td>68.2</td>\n",
       "      <td>dc</td>\n",
       "      <td>86.5</td>\n",
       "      <td>man</td>\n",
       "      <td>61.1</td>\n",
       "      <td>report</td>\n",
       "      <td>77.8</td>\n",
       "      <td>video</td>\n",
       "      <td>55.9</td>\n",
       "      <td>cold</td>\n",
       "      <td>77.1</td>\n",
       "      <td>make</td>\n",
       "      <td>74.5</td>\n",
       "      <td>take</td>\n",
       "      <td>96.8</td>\n",
       "      <td>immigr</td>\n",
       "      <td>77.7</td>\n",
       "      <td>talk</td>\n",
       "      <td>94.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>trial</td>\n",
       "      <td>68.1</td>\n",
       "      <td>think</td>\n",
       "      <td>83.3</td>\n",
       "      <td>made</td>\n",
       "      <td>54.0</td>\n",
       "      <td>save</td>\n",
       "      <td>61.8</td>\n",
       "      <td>place</td>\n",
       "      <td>54.1</td>\n",
       "      <td>call</td>\n",
       "      <td>76.5</td>\n",
       "      <td>impact</td>\n",
       "      <td>67.5</td>\n",
       "      <td>climat</td>\n",
       "      <td>76.7</td>\n",
       "      <td>obama</td>\n",
       "      <td>75.6</td>\n",
       "      <td>graham</td>\n",
       "      <td>92.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>clinic</td>\n",
       "      <td>68.1</td>\n",
       "      <td>due</td>\n",
       "      <td>80.1</td>\n",
       "      <td>show</td>\n",
       "      <td>51.9</td>\n",
       "      <td>money</td>\n",
       "      <td>56.1</td>\n",
       "      <td>good</td>\n",
       "      <td>52.6</td>\n",
       "      <td>#tcot</td>\n",
       "      <td>75.3</td>\n",
       "      <td>report</td>\n",
       "      <td>63.4</td>\n",
       "      <td>iceland</td>\n",
       "      <td>67.1</td>\n",
       "      <td>feder</td>\n",
       "      <td>67.1</td>\n",
       "      <td>un</td>\n",
       "      <td>77.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>carbon</td>\n",
       "      <td>64.5</td>\n",
       "      <td>make</td>\n",
       "      <td>70.4</td>\n",
       "      <td>water</td>\n",
       "      <td>50.6</td>\n",
       "      <td>live</td>\n",
       "      <td>49.3</td>\n",
       "      <td>human</td>\n",
       "      <td>48.2</td>\n",
       "      <td>one</td>\n",
       "      <td>72.1</td>\n",
       "      <td>issu</td>\n",
       "      <td>58.8</td>\n",
       "      <td>could</td>\n",
       "      <td>64.5</td>\n",
       "      <td>via</td>\n",
       "      <td>58.6</td>\n",
       "      <td>put</td>\n",
       "      <td>68.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic 0 words Topic 0 weights Topic 1 words Topic 1 weights Topic 2 words  \\\n",
       "0        climat          1220.2        global           666.5        global   \n",
       "1         chang          1184.5          warm           658.1          warm   \n",
       "2           via           257.9          snow           160.5     scientist   \n",
       "3        scienc           112.9         #tcot           121.6           say   \n",
       "4          news            79.5          like            99.0        scienc   \n",
       "5           day            77.8      blizzard            90.9         debat   \n",
       "6         earth            68.2            dc            86.5           man   \n",
       "7         trial            68.1         think            83.3          made   \n",
       "8        clinic            68.1           due            80.1          show   \n",
       "9        carbon            64.5          make            70.4         water   \n",
       "\n",
       "  Topic 2 weights Topic 3 words Topic 3 weights Topic 4 words Topic 4 weights  \\\n",
       "0          1147.2        global           473.1        climat           422.0   \n",
       "1          1102.1          warm           450.7         chang           401.8   \n",
       "2           150.2        believ           101.3        legisl           123.2   \n",
       "3            87.1    california            87.1            us           105.1   \n",
       "4            71.7         blame            82.1           via            60.5   \n",
       "5            66.6           law            78.6           say            55.9   \n",
       "6            61.1        report            77.8         video            55.9   \n",
       "7            54.0          save            61.8         place            54.1   \n",
       "8            51.9         money            56.1          good            52.6   \n",
       "9            50.6          live            49.3         human            48.2   \n",
       "\n",
       "  Topic 5 words Topic 5 weights Topic 6 words Topic 6 weights Topic 7 words  \\\n",
       "0        global           783.0         chang           666.1          warm   \n",
       "1          warm           764.7        climat           661.6      #climate   \n",
       "2          gore           137.1        energi           178.8       volcano   \n",
       "3          snow           123.7             u           109.8        global   \n",
       "4            al           122.1           via            96.5         chang   \n",
       "5         great            94.1        nation            89.0          help   \n",
       "6          cold            77.1          make            74.5          take   \n",
       "7          call            76.5        impact            67.5        climat   \n",
       "8         #tcot            75.3        report            63.4       iceland   \n",
       "9           one            72.1          issu            58.8         could   \n",
       "\n",
       "  Topic 7 weights Topic 8 words Topic 8 weights Topic 9 words Topic 9 weights  \n",
       "0           167.9        climat           568.3        climat           529.9  \n",
       "1           139.2         chang           550.5         chang           520.6  \n",
       "2           128.9           new           321.1         peopl           153.4  \n",
       "3           128.6          bill           116.9         world           151.3  \n",
       "4           122.0         senat           106.1        confer           110.1  \n",
       "5           114.1         agenc            95.1          bill           106.4  \n",
       "6            96.8        immigr            77.7          talk            94.8  \n",
       "7            76.7         obama            75.6        graham            92.1  \n",
       "8            67.1         feder            67.1            un            77.6  \n",
       "9            64.5           via            58.6           put            68.8  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "no_top_words = 10\n",
    "display_topics(model, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c2bea191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "model_2 = NMF(n_components=10, random_state=0, alpha=.1, l1_ratio=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19c90d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ekmho\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1477: FutureWarning: `alpha` was deprecated in version 1.0 and will be removed in 1.2. Use `alpha_W` and `alpha_H` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\ekmho\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:1692: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NMF(alpha=0.1, l1_ratio=0.5, n_components=10, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NMF</label><div class=\"sk-toggleable__content\"><pre>NMF(alpha=0.1, l1_ratio=0.5, n_components=10, random_state=0)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NMF(alpha=0.1, l1_ratio=0.5, n_components=10, random_state=0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "625c2c35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic 0 words</th>\n",
       "      <th>Topic 0 weights</th>\n",
       "      <th>Topic 1 words</th>\n",
       "      <th>Topic 1 weights</th>\n",
       "      <th>Topic 2 words</th>\n",
       "      <th>Topic 2 weights</th>\n",
       "      <th>Topic 3 words</th>\n",
       "      <th>Topic 3 weights</th>\n",
       "      <th>Topic 4 words</th>\n",
       "      <th>Topic 4 weights</th>\n",
       "      <th>Topic 5 words</th>\n",
       "      <th>Topic 5 weights</th>\n",
       "      <th>Topic 6 words</th>\n",
       "      <th>Topic 6 weights</th>\n",
       "      <th>Topic 7 words</th>\n",
       "      <th>Topic 7 weights</th>\n",
       "      <th>Topic 8 words</th>\n",
       "      <th>Topic 8 weights</th>\n",
       "      <th>Topic 9 words</th>\n",
       "      <th>Topic 9 weights</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>climat</td>\n",
       "      <td>11.1</td>\n",
       "      <td>global</td>\n",
       "      <td>9.3</td>\n",
       "      <td>via</td>\n",
       "      <td>7.7</td>\n",
       "      <td>snow</td>\n",
       "      <td>5.4</td>\n",
       "      <td>bill</td>\n",
       "      <td>5.2</td>\n",
       "      <td>new</td>\n",
       "      <td>6.0</td>\n",
       "      <td>#climate</td>\n",
       "      <td>4.9</td>\n",
       "      <td>peopl</td>\n",
       "      <td>3.8</td>\n",
       "      <td>say</td>\n",
       "      <td>4.4</td>\n",
       "      <td>#tcot</td>\n",
       "      <td>5.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>chang</td>\n",
       "      <td>10.1</td>\n",
       "      <td>warm</td>\n",
       "      <td>9.0</td>\n",
       "      <td>news</td>\n",
       "      <td>1.5</td>\n",
       "      <td>dc</td>\n",
       "      <td>1.9</td>\n",
       "      <td>senat</td>\n",
       "      <td>2.5</td>\n",
       "      <td>agenc</td>\n",
       "      <td>1.9</td>\n",
       "      <td>chang</td>\n",
       "      <td>4.6</td>\n",
       "      <td>world</td>\n",
       "      <td>3.4</td>\n",
       "      <td>report</td>\n",
       "      <td>3.3</td>\n",
       "      <td>#p</td>\n",
       "      <td>1.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>energi</td>\n",
       "      <td>0.4</td>\n",
       "      <td>make</td>\n",
       "      <td>0.3</td>\n",
       "      <td>humanitarian</td>\n",
       "      <td>0.6</td>\n",
       "      <td>gore</td>\n",
       "      <td>1.4</td>\n",
       "      <td>graham</td>\n",
       "      <td>1.6</td>\n",
       "      <td>obama</td>\n",
       "      <td>1.2</td>\n",
       "      <td>#global</td>\n",
       "      <td>1.0</td>\n",
       "      <td>earth</td>\n",
       "      <td>2.3</td>\n",
       "      <td>caus</td>\n",
       "      <td>2.4</td>\n",
       "      <td>#teaparty</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>scienc</td>\n",
       "      <td>0.4</td>\n",
       "      <td>could</td>\n",
       "      <td>0.2</td>\n",
       "      <td>chang</td>\n",
       "      <td>0.4</td>\n",
       "      <td>al</td>\n",
       "      <td>1.3</td>\n",
       "      <td>put</td>\n",
       "      <td>1.2</td>\n",
       "      <td>feder</td>\n",
       "      <td>1.2</td>\n",
       "      <td>#eco</td>\n",
       "      <td>0.8</td>\n",
       "      <td>confer</td>\n",
       "      <td>2.2</td>\n",
       "      <td>scientist</td>\n",
       "      <td>1.8</td>\n",
       "      <td>#gop</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>fight</td>\n",
       "      <td>0.3</td>\n",
       "      <td>scienc</td>\n",
       "      <td>0.2</td>\n",
       "      <td>india</td>\n",
       "      <td>0.3</td>\n",
       "      <td>mean</td>\n",
       "      <td>1.2</td>\n",
       "      <td>limbo</td>\n",
       "      <td>0.8</td>\n",
       "      <td>form</td>\n",
       "      <td>1.0</td>\n",
       "      <td>warm</td>\n",
       "      <td>0.7</td>\n",
       "      <td>right</td>\n",
       "      <td>1.3</td>\n",
       "      <td>us</td>\n",
       "      <td>1.0</td>\n",
       "      <td>scam</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>legisl</td>\n",
       "      <td>0.3</td>\n",
       "      <td>think</td>\n",
       "      <td>0.2</td>\n",
       "      <td>com</td>\n",
       "      <td>0.2</td>\n",
       "      <td>jr</td>\n",
       "      <td>1.0</td>\n",
       "      <td>exit</td>\n",
       "      <td>0.8</td>\n",
       "      <td>studi</td>\n",
       "      <td>0.7</td>\n",
       "      <td>us</td>\n",
       "      <td>0.4</td>\n",
       "      <td>bolivia</td>\n",
       "      <td>1.0</td>\n",
       "      <td>govern</td>\n",
       "      <td>0.8</td>\n",
       "      <td>dc</td>\n",
       "      <td>0.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>u</td>\n",
       "      <td>0.3</td>\n",
       "      <td>stop</td>\n",
       "      <td>0.2</td>\n",
       "      <td>un</td>\n",
       "      <td>0.1</td>\n",
       "      <td>storm</td>\n",
       "      <td>1.0</td>\n",
       "      <td>climat</td>\n",
       "      <td>0.7</td>\n",
       "      <td>administr</td>\n",
       "      <td>0.7</td>\n",
       "      <td>fact</td>\n",
       "      <td>0.3</td>\n",
       "      <td>day</td>\n",
       "      <td>1.0</td>\n",
       "      <td>may</td>\n",
       "      <td>0.8</td>\n",
       "      <td>gore</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>un</td>\n",
       "      <td>0.3</td>\n",
       "      <td>effect</td>\n",
       "      <td>0.2</td>\n",
       "      <td>environ</td>\n",
       "      <td>0.1</td>\n",
       "      <td>cold</td>\n",
       "      <td>0.9</td>\n",
       "      <td>compromis</td>\n",
       "      <td>0.7</td>\n",
       "      <td>propos</td>\n",
       "      <td>0.6</td>\n",
       "      <td>nasa</td>\n",
       "      <td>0.3</td>\n",
       "      <td>mother</td>\n",
       "      <td>1.0</td>\n",
       "      <td>u</td>\n",
       "      <td>0.4</td>\n",
       "      <td>#ocra</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>get</td>\n",
       "      <td>0.2</td>\n",
       "      <td>volcano</td>\n",
       "      <td>0.2</td>\n",
       "      <td>mean</td>\n",
       "      <td>0.1</td>\n",
       "      <td>washington</td>\n",
       "      <td>0.9</td>\n",
       "      <td>prepar</td>\n",
       "      <td>0.7</td>\n",
       "      <td>york</td>\n",
       "      <td>0.6</td>\n",
       "      <td>#p</td>\n",
       "      <td>0.3</td>\n",
       "      <td>indigen</td>\n",
       "      <td>0.4</td>\n",
       "      <td>carbon</td>\n",
       "      <td>0.3</td>\n",
       "      <td>al</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>action</td>\n",
       "      <td>0.2</td>\n",
       "      <td>believ</td>\n",
       "      <td>0.2</td>\n",
       "      <td>impact</td>\n",
       "      <td>0.1</td>\n",
       "      <td>ago</td>\n",
       "      <td>0.9</td>\n",
       "      <td>immigr</td>\n",
       "      <td>0.6</td>\n",
       "      <td>report</td>\n",
       "      <td>0.5</td>\n",
       "      <td>thought</td>\n",
       "      <td>0.3</td>\n",
       "      <td>cochabamba</td>\n",
       "      <td>0.3</td>\n",
       "      <td>china</td>\n",
       "      <td>0.3</td>\n",
       "      <td>#tlot</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Topic 0 words Topic 0 weights Topic 1 words Topic 1 weights Topic 2 words  \\\n",
       "0        climat            11.1        global             9.3           via   \n",
       "1         chang            10.1          warm             9.0          news   \n",
       "2        energi             0.4          make             0.3  humanitarian   \n",
       "3        scienc             0.4         could             0.2         chang   \n",
       "4         fight             0.3        scienc             0.2         india   \n",
       "5        legisl             0.3         think             0.2           com   \n",
       "6             u             0.3          stop             0.2            un   \n",
       "7            un             0.3        effect             0.2       environ   \n",
       "8           get             0.2       volcano             0.2          mean   \n",
       "9        action             0.2        believ             0.2        impact   \n",
       "\n",
       "  Topic 2 weights Topic 3 words Topic 3 weights Topic 4 words Topic 4 weights  \\\n",
       "0             7.7          snow             5.4          bill             5.2   \n",
       "1             1.5            dc             1.9         senat             2.5   \n",
       "2             0.6          gore             1.4        graham             1.6   \n",
       "3             0.4            al             1.3           put             1.2   \n",
       "4             0.3          mean             1.2         limbo             0.8   \n",
       "5             0.2            jr             1.0          exit             0.8   \n",
       "6             0.1         storm             1.0        climat             0.7   \n",
       "7             0.1          cold             0.9     compromis             0.7   \n",
       "8             0.1    washington             0.9        prepar             0.7   \n",
       "9             0.1           ago             0.9        immigr             0.6   \n",
       "\n",
       "  Topic 5 words Topic 5 weights Topic 6 words Topic 6 weights Topic 7 words  \\\n",
       "0           new             6.0      #climate             4.9         peopl   \n",
       "1         agenc             1.9         chang             4.6         world   \n",
       "2         obama             1.2       #global             1.0         earth   \n",
       "3         feder             1.2          #eco             0.8        confer   \n",
       "4          form             1.0          warm             0.7         right   \n",
       "5         studi             0.7            us             0.4       bolivia   \n",
       "6     administr             0.7          fact             0.3           day   \n",
       "7        propos             0.6          nasa             0.3        mother   \n",
       "8          york             0.6            #p             0.3       indigen   \n",
       "9        report             0.5       thought             0.3    cochabamba   \n",
       "\n",
       "  Topic 7 weights Topic 8 words Topic 8 weights Topic 9 words Topic 9 weights  \n",
       "0             3.8           say             4.4         #tcot             5.6  \n",
       "1             3.4        report             3.3            #p             1.7  \n",
       "2             2.3          caus             2.4     #teaparty             1.0  \n",
       "3             2.2     scientist             1.8          #gop             0.6  \n",
       "4             1.3            us             1.0          scam             0.6  \n",
       "5             1.0        govern             0.8            dc             0.6  \n",
       "6             1.0           may             0.8          gore             0.5  \n",
       "7             1.0             u             0.4         #ocra             0.5  \n",
       "8             0.4        carbon             0.3            al             0.5  \n",
       "9             0.3         china             0.3         #tlot             0.5  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_topics(model_2, tf_feature_names, no_top_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5add8f38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef18612",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
