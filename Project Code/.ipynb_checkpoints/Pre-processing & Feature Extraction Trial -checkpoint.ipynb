{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6332f439",
   "metadata": {},
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afef1b3d",
   "metadata": {},
   "source": [
    "The first section below uses snscrape to get a list of tweets\n",
    "- I was aiming to try to use this to get tweets to practise preprocessing on but since I'm not sure out dataset will be laid out in the same way (same pandas dataframe) I will jsut try with a dataset of 20,000 random tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f5bdd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (0.5.0.20230113)\n",
      "Requirement already satisfied: filelock in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from snscrape) (3.6.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from snscrape) (4.9.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from snscrape) (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from snscrape) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from beautifulsoup4->snscrape) (2.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (3.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "42728899",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'textblob'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msnscrape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodules\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtwitter\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msntwitter\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtextblob\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TextBlob \n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m twitter_samples\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'textblob'"
     ]
    }
   ],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from textblob import TextBlob \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "\n",
    "from argparse import Namespace\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ef1eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7490600",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:elonmusk').get_items()):\n",
    "    if i>=100:\n",
    "        break\n",
    "    tweets_list.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a0c2800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f780888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_1 = df.iloc[0,:] # Selecting the first row of the dataset to analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f2380",
   "metadata": {},
   "source": [
    "# Section 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337cc56b",
   "metadata": {},
   "source": [
    "Here I will try to use the NLTK library to do data preprocessing on the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfb855f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ekmho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "english_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c3b9631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\ekmho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0368c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = twitter_samples.strings(\"tweets.20150430-223406.json\")\n",
    "all_tweets = pd.Series(all_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1131f43d",
   "metadata": {},
   "source": [
    "<b>Finding retweets<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e95916f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tokens = all_tweets.str.split(expand=True)[0]\n",
    "retweeted = all_tweets[all_tweets.str.startswith(\"RT\")].value_counts().iloc[:10] # These are all the retweeted tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab0f64",
   "metadata": {},
   "source": [
    "<b>Filter English tweets</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "textBlob('your tweet').detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b3988",
   "metadata": {},
   "source": [
    "<b> Remove Capitalisation (can leave a marker)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "marker = false\n",
    "for tweet in tweets: \n",
    "    tweet = tweet.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e69504",
   "metadata": {},
   "source": [
    "<b>Clean whitespace, URLS, punctuation, hashtags, emojis</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf3ede",
   "metadata": {},
   "source": [
    "Below it is shown how to remove unwanted phrases or tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6747f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_OF_LINE = r\"^\"\n",
    "OPTIONAL = \"?\"\n",
    "ANYTHING = \".\"\n",
    "ZERO_OR_MORE = \"*\"\n",
    "ONE_OR_MORE = \"+\"\n",
    "\n",
    "SPACE = \"\\s\"\n",
    "SPACES = SPACE + ONE_OR_MORE\n",
    "NOT_SPACE = \"[^\\s]\" + ONE_OR_MORE\n",
    "EVERYTHING_OR_NOTHING = ANYTHING + ZERO_OR_MORE\n",
    "\n",
    "ERASE = \"\"\n",
    "FORWARD_SLASH = \"\\/\"\n",
    "NEWLINES = r\"[\\r\\n]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_TWEET = START_OF_LINE + \"RT\" + SPACES\n",
    "HYPERLINKS = (\"http\" + \"s\" + OPTIONAL + \":\" + FORWARD_SLASH + FORWARD_SLASH\n",
    "              + NOT_SPACE + NEWLINES + ZERO_OR_MORE)\n",
    "HASH = \"#\"\n",
    "tweet = re.sub(RE_TWEET, ERASE, tweet) # tweet is the individual tweet - this can also be done through pandas\n",
    "#Can also sub in any values you want to remove in the above line of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33760dcb",
   "metadata": {},
   "source": [
    "<b>Tokenisation<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f237641",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(#enter list of tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dd8676",
   "metadata": {},
   "source": [
    "<b>Named Entity Recognition</b>\n",
    "- Same step as removal of unwanted phrases - named entities start with an @"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925452f2",
   "metadata": {},
   "source": [
    "<b>Stemming</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa1347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in #list of clean tweets]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23b956",
   "metadata": {},
   "source": [
    "<b>Remove stopwords and punctuation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4032ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = [word for word in #list of tweets if (word not in english_stopwords and\n",
    "                                       word not in string.punctuation)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
