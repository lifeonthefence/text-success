{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6332f439",
   "metadata": {},
   "source": [
    "# Section 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afef1b3d",
   "metadata": {},
   "source": [
    "The first section below uses snscrape to get a list of tweets\n",
    "- I was aiming to try to use this to get tweets to practise preprocessing on but since I'm not sure out dataset will be laid out in the same way (same pandas dataframe) I will jsut try with a dataset of 20,000 random tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1f5bdd31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: snscrape in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (0.5.0.20230113)\n",
      "Requirement already satisfied: filelock in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from snscrape) (3.6.0)\n",
      "Requirement already satisfied: lxml in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from snscrape) (4.9.1)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from snscrape) (2.28.1)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from snscrape) (4.11.1)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from beautifulsoup4->snscrape) (2.3.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2022.12.7)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (3.3)\n",
      "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from requests[socks]->snscrape) (1.7.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install snscrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d66cc65b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.17.1-py2.py3-none-any.whl (636 kB)\n",
      "     -------------------------------------- 636.8/636.8 kB 6.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from textblob) (3.7)\n",
      "Requirement already satisfied: tqdm in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (4.64.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (2022.7.9)\n",
      "Requirement already satisfied: click in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (8.0.4)\n",
      "Requirement already satisfied: joblib in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from nltk>=3.1->textblob) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\ekmho\\anaconda3\\lib\\site-packages (from click->nltk>=3.1->textblob) (0.4.5)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.17.1\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "42728899",
   "metadata": {},
   "outputs": [],
   "source": [
    "import snscrape.modules.twitter as sntwitter\n",
    "import pandas as pd\n",
    "from textblob import TextBlob \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import twitter_samples\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "import nltk\n",
    "\n",
    "from argparse import Namespace\n",
    "from functools import partial\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ef1eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_list = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7490600",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,tweet in enumerate(sntwitter.TwitterSearchScraper('from:elonmusk').get_items()):\n",
    "    if i>=100:\n",
    "        break\n",
    "    tweets_list.append(tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2a0c2800",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(tweets_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f780888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "row_1 = df.iloc[0,:] # Selecting the first row of the dataset to analyse"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989f2380",
   "metadata": {},
   "source": [
    "# Section 2 - Data Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337cc56b",
   "metadata": {},
   "source": [
    "Here the NLTK library is used to do data prepocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "cfb855f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ekmho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "english_stopwords = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9c3b9631",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     C:\\Users\\ekmho\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('twitter_samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0368c7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tweets = twitter_samples.strings(\"tweets.20150430-223406.json\")\n",
    "all_tweets = pd.Series(all_tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1131f43d",
   "metadata": {},
   "source": [
    "<b>Finding retweets<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e95916f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_tokens = all_tweets.str.split(expand=True)[0]\n",
    "retweeted = all_tweets[all_tweets.str.startswith(\"RT\")].value_counts().iloc[:10] # These are all the retweeted tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ab0f64",
   "metadata": {},
   "source": [
    "<b>Filter English tweets</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2586b433",
   "metadata": {},
   "outputs": [],
   "source": [
    "textBlob('your tweet').detect_language()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1b3988",
   "metadata": {},
   "source": [
    "<b> Remove Capitalisation (can leave a marker)</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ed3b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "marker = false\n",
    "for tweet in tweets: \n",
    "    tweet = tweet.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e69504",
   "metadata": {},
   "source": [
    "<b>Clean whitespace, URLS, punctuation, hashtags, emojis</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02bf3ede",
   "metadata": {},
   "source": [
    "Below it is shown how to remove unwanted phrases or tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6747f345",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_OF_LINE = r\"^\"\n",
    "OPTIONAL = \"?\"\n",
    "ANYTHING = \".\"\n",
    "ZERO_OR_MORE = \"*\"\n",
    "ONE_OR_MORE = \"+\"\n",
    "\n",
    "SPACE = \"\\s\"\n",
    "SPACES = SPACE + ONE_OR_MORE\n",
    "NOT_SPACE = \"[^\\s]\" + ONE_OR_MORE\n",
    "EVERYTHING_OR_NOTHING = ANYTHING + ZERO_OR_MORE\n",
    "\n",
    "ERASE = \"\"\n",
    "FORWARD_SLASH = \"\\/\"\n",
    "NEWLINES = r\"[\\r\\n]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd96a8c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RE_TWEET = START_OF_LINE + \"RT\" + SPACES\n",
    "HYPERLINKS = (\"http\" + \"s\" + OPTIONAL + \":\" + FORWARD_SLASH + FORWARD_SLASH\n",
    "              + NOT_SPACE + NEWLINES + ZERO_OR_MORE)\n",
    "HASH = \"#\"\n",
    "tweet = re.sub(RE_TWEET, ERASE, tweet) # tweet is the individual tweet - this can also be done through pandas\n",
    "#Can also sub in any values you want to remove in the above line of code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35526554",
   "metadata": {},
   "source": [
    "Here is another way to remove unnecessary words if the data comes as a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eeb1c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the re library\n",
    "token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50da9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['processed_text'] = df['text'].str.lower()\\\n",
    "          .str.replace('(@[a-z0-9]+)\\w+',' ')\\\n",
    "          .str.replace('(http\\S+)', ' ')\\\n",
    "          .str.replace('([^0-9a-z \\t])',' ')\\\n",
    "          .str.replace(' +',' ')\\\n",
    "          .apply(lambda x: [i for i in x.split() if not i in swords])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f096bc57",
   "metadata": {},
   "source": [
    "<b>Stopwords</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a3cd55c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33760dcb",
   "metadata": {},
   "source": [
    "<b>Tokenisation<b/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f237641",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.tokenize(#enter list of tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73dd8676",
   "metadata": {},
   "source": [
    "<b>Named Entity Recognition</b>\n",
    "- Same step as removal of unwanted phrases - named entities start with an @"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925452f2",
   "metadata": {},
   "source": [
    "<b>Stemming</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa1347e",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = PorterStemmer()\n",
    "stemmed = [stemmer.stem(word) for word in #list of clean tweets]\n",
    "#Or with a dataframe it would look like\n",
    "df['stemmed'] = df['processed_text'].apply(lambda x: [ps.stem(i) for i in x if i != ''])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e23b956",
   "metadata": {},
   "source": [
    "<b>Remove stopwords and punctuation</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4032ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned = [word for word in #list of tweets if (word not in english_stopwords and\n",
    "                                       word not in string.punctuation)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870ec3ee",
   "metadata": {},
   "source": [
    "<b>Tagging</b> - This labels each word in the tokenized text with word type "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a103734c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag\n",
    "\n",
    "tagged_words = pos_tag(#variable for tokenized words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29435f7c",
   "metadata": {},
   "source": [
    "<b>Lemmatization</b> - This is an alternative way to stemming to normalise words (stemming gets the root of the word, lemmatization use context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f588488",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "def lemmatize_sentence(tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_sentence = []\n",
    "    for word, tag in pos_tag(tokens):\n",
    "        if tag.startswith('NN'):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "        lemmatized_sentence.append(lemmatizer.lemmatize(word, pos))\n",
    "    return lemmatized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077260c8",
   "metadata": {},
   "source": [
    "# Section 3 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74939cbd",
   "metadata": {},
   "source": [
    "- Hashtags, URLs and mentions can be extracted during the data cleaning process above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a780bc2",
   "metadata": {},
   "source": [
    "<b>Emojis</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1098864",
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_count = 0\n",
    "\n",
    "for each character in text: \n",
    "    unicode_num = ord(character)\n",
    "    if (unicode num >= 8986) and if (unicode)num <= 129510): \n",
    "        emoji_count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1bbe4e0",
   "metadata": {},
   "source": [
    "<b>Sentiment analysis</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab2fa20",
   "metadata": {},
   "source": [
    "<b>Time of day</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa973b4",
   "metadata": {},
   "source": [
    "<b>Speed of tweet after event</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75504d2",
   "metadata": {},
   "source": [
    "<b>Length of text</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ee445f",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = len(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aeffcb",
   "metadata": {},
   "source": [
    "<b>TF-IDF: term frequency - inverse document frequency</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359e834e",
   "metadata": {},
   "source": [
    "<b>Identifying named people</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d857ecd",
   "metadata": {},
   "source": [
    "One way to do this would be to have a list of key people that we want to check for and then comparing the text tokens to these names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec5228e",
   "metadata": {},
   "source": [
    "<b>User level features</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b062bf8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
